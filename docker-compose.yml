services:
  # Excalidraw AI Agent web application
  excalidraw-app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        # ===========================================
        # AI Provider Configuration
        # ===========================================
        VITE_AI_PROVIDER: ${VITE_AI_PROVIDER:-anthropic}

        # ===========================================
        # Anthropic (Claude) Configuration
        # ===========================================
        VITE_ANTHROPIC_API_KEY: ${VITE_ANTHROPIC_API_KEY}
        VITE_ANTHROPIC_MODEL: ${VITE_ANTHROPIC_MODEL:-claude-sonnet-4-20250514}

        # ===========================================
        # Ollama Configuration (Local LLM)
        # ===========================================
        VITE_OLLAMA_BASE_URL: ${VITE_OLLAMA_BASE_URL:-http://ollama:11434}
        VITE_OLLAMA_MODEL: ${VITE_OLLAMA_MODEL:-llama3.2}

        # ===========================================
        # Model Parameters (applies to all providers)
        # ===========================================
        VITE_AI_MAX_TOKENS: ${VITE_AI_MAX_TOKENS:-4096}
        VITE_AI_TEMPERATURE: ${VITE_AI_TEMPERATURE:-0.7}
        VITE_AI_TOP_P: ${VITE_AI_TOP_P:-0.9}
        VITE_AI_TOP_K: ${VITE_AI_TOP_K:-40}
        VITE_AI_REPEAT_PENALTY: ${VITE_AI_REPEAT_PENALTY:-1.1}
        VITE_AI_MAX_RETRIES: ${VITE_AI_MAX_RETRIES:-3}
        VITE_AI_RETRY_DELAY: ${VITE_AI_RETRY_DELAY:-1000}

        # ===========================================
        # Debug & Logging Configuration
        # ===========================================
        VITE_AI_DEBUG: ${VITE_AI_DEBUG:-false}
        VITE_LOGGING_ENABLED: ${VITE_LOGGING_ENABLED:-true}
        VITE_LOG_TO_CONSOLE: ${VITE_LOG_TO_CONSOLE:-false}
        VITE_MAX_LOG_ENTRIES: ${VITE_MAX_LOG_ENTRIES:-500}

        # ===========================================
        # N8N Webhook Integration Configuration
        # ===========================================
        VITE_N8N_WEBHOOK_URL: ${VITE_N8N_WEBHOOK_URL:-}
        VITE_N8N_WEBHOOK_TOKEN: ${VITE_N8N_WEBHOOK_TOKEN:-}
        VITE_N8N_DEBUG: ${VITE_N8N_DEBUG:-false}
        VITE_N8N_CALLBACK_URL: ${VITE_N8N_CALLBACK_URL:-}
    container_name: excalidraw-ai-agent
    restart: unless-stopped
    ports:
      - "8080:80"
    environment:
      # ===========================================
      # AI Provider Configuration
      # ===========================================

      # AI Provider: 'anthropic' or 'ollama'
      VITE_AI_PROVIDER: ${VITE_AI_PROVIDER:-anthropic}

      # ===========================================
      # Anthropic (Claude) Configuration
      # ===========================================

      # Anthropic API Key for Claude AI integration
      VITE_ANTHROPIC_API_KEY: ${VITE_ANTHROPIC_API_KEY}

      # Claude model to use
      # Options:
      #   claude-opus-4-5-20251101   — Claude Opus 4.5 (most capable)
      #   claude-sonnet-4-5-20250929 — Claude Sonnet 4.5 (balanced)
      #   claude-haiku-4-5-20251001  — Claude Haiku 4.5 (fastest)
      VITE_ANTHROPIC_MODEL: ${VITE_ANTHROPIC_MODEL:-claude-sonnet-4-20250514}

      # ===========================================
      # Ollama Configuration (Local LLM)
      # ===========================================

      # Ollama API base URL (default: http://localhost:11434)
      VITE_OLLAMA_BASE_URL: ${VITE_OLLAMA_BASE_URL:-http://ollama:11434}

      # Ollama model to use
      # Run 'ollama list' to see available models
      # Popular options: llama3.2, mistral, codellama, phi3
      VITE_OLLAMA_MODEL: ${VITE_OLLAMA_MODEL:-llama3.2}

      # ===========================================
      # Model Parameters (applies to all providers)
      # ===========================================

      # Maximum tokens in response (default: 4096)
      VITE_AI_MAX_TOKENS: ${VITE_AI_MAX_TOKENS:-4096}

      # Temperature: Controls randomness (0.0-1.0)
      # Lower = more focused, Higher = more creative
      # Default: 0.7
      VITE_AI_TEMPERATURE: ${VITE_AI_TEMPERATURE:-0.7}

      # Top P: Nucleus sampling threshold (0.0-1.0)
      # Default: 0.9
      VITE_AI_TOP_P: ${VITE_AI_TOP_P:-0.9}

      # Top K: Limits vocabulary for each step (Ollama only)
      # Default: 40
      VITE_AI_TOP_K: ${VITE_AI_TOP_K:-40}

      # Repeat Penalty: Penalizes repetition (Ollama only)
      # Default: 1.1
      VITE_AI_REPEAT_PENALTY: ${VITE_AI_REPEAT_PENALTY:-1.1}

      # Seed: For reproducible outputs (optional, Ollama only)
      # Uncomment to enable
      # VITE_AI_SEED: ${VITE_AI_SEED:-42}

      # ===========================================
      # Retry Configuration
      # ===========================================

      # Maximum retry attempts for AI requests when response is empty or invalid format
      # Default: 3
      VITE_AI_MAX_RETRIES: ${VITE_AI_MAX_RETRIES:-3}

      # Delay between retries in milliseconds
      # Default: 1000 (1 second)
      VITE_AI_RETRY_DELAY: ${VITE_AI_RETRY_DELAY:-1000}

      # ===========================================
      # Debug & Logging Configuration
      # ===========================================

      # Enable AI service debug logging
      # Shows detailed request/response info in console
      VITE_AI_DEBUG: ${VITE_AI_DEBUG:-false}

      # Enable canvas and AI event logging
      # Logs to localStorage and console for cognitive process learning
      VITE_LOGGING_ENABLED: ${VITE_LOGGING_ENABLED:-true}

      # Enable console output for logging service
      VITE_LOG_TO_CONSOLE: ${VITE_LOG_TO_CONSOLE:-false}

      # Maximum log entries to keep (default: 500)
      VITE_MAX_LOG_ENTRIES: ${VITE_MAX_LOG_ENTRIES:-500}

      # ===========================================
      # N8N Webhook Integration Configuration
      # ===========================================

      # N8N Webhook URL for sending selected canvas elements
      # Example: https://your-n8n-instance.com/webhook/excalidraw
      # Leave empty to disable N8N webhook feature
      VITE_N8N_WEBHOOK_URL: ${VITE_N8N_WEBHOOK_URL:-}

      # N8N Webhook authentication token (optional)
      # If your N8N webhook requires authentication
      # VITE_N8N_WEBHOOK_TOKEN: ${VITE_N8N_WEBHOOK_TOKEN:-}

      # Enable N8N webhook debug logging
      VITE_N8N_DEBUG: ${VITE_N8N_DEBUG:-false}

      # N8N Callback URL (auto-generated, for reference)
      # The app listens for updates from N8N at this URL:
      # http://localhost:5173/api/n8n/callback
      #
      # In Docker, use the service name:
      # http://excalidraw-app:8080/api/n8n/callback
      #
      # For external access, use your public URL or local IP:
      # http://your-server-ip:8080/api/n8n/callback

      # Optional: Override the callback URL (useful for remote N8N instances)
      # If not set, the URL is auto-generated as: http://localhost:5173/api/n8n/callback
      # Set this to your public URL or local IP for remote N8N access
      # VITE_N8N_CALLBACK_URL: ${VITE_N8N_CALLBACK_URL:-}
    networks:
      - excalidraw-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ===========================================
  # Optional: Ollama Service (Local LLM)
  # Uncomment this section if you want to run Ollama
  # in the same Docker Compose setup
  # ===========================================

  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   restart: unless-stopped
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   networks:
  #     - excalidraw-network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #           capabilities: [gpu]

  # ===========================================
  # Optional: N8N Service (Workflow Automation)
  # Uncomment this section if you want to run N8N
  # in the same Docker Compose setup
  # ===========================================

  # n8n:
  #   image: n8nio/n8n:latest
  #   container_name: n8n
  #   restart: unless-stopped
  #   ports:
  #     - "5678:5678"
  #   environment:
  #     - N8N_BASIC_AUTH_ACTIVE=true
  #     - N8N_BASIC_AUTH_USER=${N8N_USER:-admin}
  #     - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD:-password}
  #     - N8N_HOST=${N8N_HOST:-n8n}
  #     - N8N_PORT=5678
  #     - WEBHOOK_URL=http://your-domain.com
  #     - GENERIC_TIMEZONE=${TIMEZONE:-UTC}
  #   volumes:
  #     - n8n-data:/home/node/.n8n
  #   networks:
  #     - excalidraw-network
  #   depends_on:
  #     - excalidraw-app

  # ===========================================
  # Optional: Redis (for caching/sessions)
  # Uncomment if your application needs Redis
  # ===========================================

  # redis:
  #   image: redis:7-alpine
  #   container_name: excalidraw-redis
  #   restart: unless-stopped
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis-data:/data
  #   networks:
  #     - excalidraw-network
  #   command: redis-server --appendonly yes

networks:
  excalidraw-network:
    driver: bridge
    name: excalidraw-ai-agent-network

# ===========================================
# Volumes for data persistence
# ===========================================
volumes:
  ollama-data:
    driver: local
    name: excalidraw-ollama-data

  n8n-data:
    driver: local
    name: excalidraw-n8n-data

  redis-data:
    driver: local
    name: excalidraw-redis-data
