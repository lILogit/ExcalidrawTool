# ===========================================
# Excalidraw AI Agent - Docker Compose Environment Variables
# ===========================================
# Copy this file to .env when deploying with Docker Compose
# Fill in the required values for your deployment

# ===========================================
# Application Configuration
# ===========================================

# Port to expose the application (default: 80)
APP_PORT=80

# ===========================================
# AI Provider Configuration
# ===========================================

# AI Provider: 'anthropic' or 'ollama'
# Default: anthropic
VITE_AI_PROVIDER=anthropic

# ===========================================
# Anthropic (Claude) Configuration
# ===========================================

# REQUIRED: Anthropic API Key for Claude AI integration
# Get your key at: https://console.anthropic.com/
VITE_ANTHROPIC_API_KEY=sk-ant-xxxxx

# Claude model to use
# Options:
#   claude-opus-4-5-20251101   — Claude Opus 4.5 (most capable)
#   claude-sonnet-4-5-20250929 — Claude Sonnet 4.5 (balanced)
#   claude-haiku-4-5-20251001  — Claude Haiku 4.5 (fastest)
# Default: claude-sonnet-4-20250514
VITE_ANTHROPIC_MODEL=claude-sonnet-4-20250514

# ===========================================
# Ollama Configuration (Local LLM)
# ===========================================

# Ollama API base URL (default: http://ollama:11434)
# When using docker-compose.yml with ollama service, use service name
VITE_OLLAMA_BASE_URL=http://ollama:11434

# Ollama model to use
# Popular options: llama3.2, mistral, codellama, phi3
VITE_OLLAMA_MODEL=llama3.2

# ===========================================
# Model Parameters (applies to all providers)
# ===========================================

# Maximum tokens in response (default: 4096)
VITE_AI_MAX_TOKENS=4096

# Temperature: Controls randomness (0.0-1.0)
# Lower = more focused, Higher = more creative
# Default: 0.7
VITE_AI_TEMPERATURE=0.7

# Top P: Nucleus sampling threshold (0.0-1.0)
# Default: 0.9
VITE_AI_TOP_P=0.9

# Top K: Limits vocabulary for each step (Ollama only)
# Default: 40
VITE_AI_TOP_K=40

# Repeat Penalty: Penalizes repetition (Ollama only)
# Default: 1.1
VITE_AI_REPEAT_PENALTY=1.1

# Seed: For reproducible outputs (optional, Ollama only)
# VITE_AI_SEED=42

# ===========================================
# Retry Configuration
# ===========================================

# Maximum retry attempts for AI requests when response is empty or invalid format
# Default: 3
VITE_AI_MAX_RETRIES=3

# Delay between retries in milliseconds
# Default: 1000 (1 second)
VITE_AI_RETRY_DELAY=1000

# ===========================================
# Debug & Logging Configuration
# ===========================================

# Enable AI service debug logging
# Shows detailed request/response info in console
# Default: false
VITE_AI_DEBUG=false

# Enable canvas and AI event logging
# Logs to localStorage and console for cognitive process learning
# Default: true
VITE_LOGGING_ENABLED=true

# Enable console output for logging service
# Default: false
VITE_LOG_TO_CONSOLE=false

# Maximum log entries to keep (default: 500)
VITE_MAX_LOG_ENTRIES=500

# ===========================================
# N8N Webhook Integration Configuration
# ===========================================

# N8N Webhook URL for sending selected canvas elements
# Example: https://your-n8n-instance.com/webhook/excalidraw
# Leave empty to disable N8N webhook feature
VITE_N8N_WEBHOOK_URL=

# N8N Webhook authentication token (optional)
# If your N8N webhook requires authentication
# VITE_N8N_WEBHOOK_TOKEN=

# Enable N8N webhook debug logging
# Default: false
VITE_N8N_DEBUG=false

# N8N Callback URL (for Docker deployments)
# The app listens for updates from N8N at this URL.
# For Docker Compose, use the service name and port:
# http://excalidraw-app:8080/api/n8n/callback
#
# For external access, use your public URL or domain:
# https://your-domain.com/api/n8n/callback
VITE_N8N_CALLBACK_URL=
